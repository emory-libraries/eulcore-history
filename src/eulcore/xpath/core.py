import logging
import os
import re
from ply import lex, yacc

from eulcore.xpath import lexrules
from eulcore.xpath import parserules
from eulcore.xpath.ast import serialize

__all__ = [ 'lexer', 'parser', 'parse', 'serialize' ]

# logging: By default ply will output a parser.out file to the program's
# working directory. To disable this, we need to specify a logger. We use
# logging.getLogger('ply'). Of course, if our calling application doesn't
# initialize logging then they'll get a weird warning when ply tries to
# write to it, so we'll set up a default do-nothing handler for that logger.
# This in no way precludes a program from using logging normally to define a
# 'real' logger for the ply module.

class NullHandler(logging.Handler):
    def emil(self, record):
        pass

logger = logging.getLogger('ply')
logger.addHandler(NullHandler)

# build the lexer. This will generate a lextab.py in the eulcore.xpath
# directory. Unfortunately the lexer generated by default by ply doesn't
# track the last token returned, which we need for xpath lexing (see the
# lexrules comments for details). To overcome that, we create a wrapper
# class that extends token() to record the last token returned, and we
# dynamically set the lexer's __class__ to this wrapper. That's pretty weird
# and ugly, but Python allows it. If you can find a prettier solution to the
# problem then I welcome a fix.
lexdir = os.path.dirname(lexrules.__file__)
lexer = lex.lex(module=lexrules, optimize=1, outputdir=lexdir, 
    debuglog=logger, errorlog=logger, reflags=re.UNICODE)

class LexerWrapper(lex.Lexer):
    def token(self):
        self.last = lex.Lexer.token(self)
        return self.last
lexer.__class__ = LexerWrapper

# build the parser. This will generate a parsetab.py in the eulcore.xpath
# directory. Other than that, it's much less exciting than the lexer
# wackiness.
parsedir = os.path.dirname(parserules.__file__)
parser = yacc.yacc(module=parserules, outputdir=parsedir,
    debuglog=logger, errorlog=logger)
parse = parser.parse

def ptokens(s):
    '''Lex a string as XPath tokens, and print each token as it is lexed.
    This is used primarily for debugging. You probably don't want this
    function.'''

    lexer.input(s)
    for tok in lexer:
            print tok

def sprint(obj, stm=None):
    '''Parse a string, and output a simple tree representation to a stream
    (defaults to sys.stdout). This is used primarily for debugging. You
    probably don't want this function.'''

    if isinstance(obj, basestring):
        obj = parser.parse(obj, lexer=lexer)

    if stm is None:
        import sys
        stm = sys.stdout
    _sprint(obj, '', stm)

def _sprint(obj, indent, stm):
    if hasattr(obj, 'struct'):
        obj = obj.struct()
    if isinstance(obj, (list, tuple)):
        print >>stm, indent + str(obj[0])
        for item in obj[1:]:
            _sprint(item, indent + '. ', stm)
    else:
        print >>stm, indent + str(obj)
